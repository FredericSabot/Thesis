\chapter{Conclusions and future work}
\label{ch:perspectives}

Society has grown to expect electricity to be available at any time and any interruption of service can thus have massive consequences both from an economic perspective (production downtime, equipment damage) and from a social perspective (injuries due to the lack of electricity, civil disorders). To meet those reliability expectations, power systems have often been designed around the N-1 security criterion that states the system should always be able to withstand the loss of any single element. However, past disturbances have shown that, although rarer, N-2 and higher order contingencies can occur and lead to cascading outages and widespread blackouts. On the other hand, the increasing share of intermittent energy sources indicate and economic pressure are pushing for TSOs to relax the N-1 criterion for system conditions that are deemed as unlikely or infrequent. There is thus an increasing willingness of TSOs to complement the deterministic N-1 criterion with probabilistic methods that can more adequately estimate the risk of blackouts and balance it with the cost of avoiding those blackouts. In this context, the objective of this thesis was to develop a probabilistic methodology to identify the scenarios most likely to lead to blackouts and to estimate their frequency and consequences. More specifically, this thesis focused on cascading outages initiated by short-term stability issues.

\section{Main Contributions}

Probabilistic methodologies require more advanced models than deterministic ones. Most importantly, protection systems play a key role in the initiation and propagation of cascading outages and have thus been studied in chapter~\ref{ch:protections}. Indeed, failure of protection systems to isolate faults can transform mild N-1 contingencies into more severe N-k ones and initiating a cascading outage. To model these failures, standard reliability techniques such as failure mode and effect analysis and event trees can be used. The application of these techniques to power systems are extensively described in~\cite{GridPSA}, and this thesis reuse this work to define contingency list to be considered in a probabilistic dynamic security assessment and to estimate the frequency of occurrence of such contingencies.

Conversely, the impact of protection systems during cascading outages in more difficult to model. Indeed, during cascading outages, and especially fast cascading outages, many protection systems might operate in a short period of time. This implies that the evolution of cascading outages can be very sensitive to the timing of protection system operations. In the literature, this sensitivity has been handled (when not disregarded) with multiple Monte Carlo methods. However, these methods are too computationally expensive to be used in systematic analysis that consider many possible contingencies in large systems. This thesis has thus proposed an indicator to predict which scenarios lead to cascading outages that are sensitive to the timing of protection systems operations and which are not, so that Monte Carlo simulations can be focused on the former.

Another class of models that require more consideration in probabilistic security assessments than deterministic ones is the models of distribution grids that were studied in chapter~\ref{ch:distrib}. Indeed, distributed energy sources tend to disconnected themselves during severe disturbances further degrading the state of the system. Similarly, motors are more likely to stall in those conditions leading to cascading effects. However, due to the low observability of distribution grids and the sheer number of elements connected to them, there are many uncertainties in the models of distribution grids. To handle those uncertainties, this thesis has proposed to build two dynamic equivalents per distribution grid model to bound the likely behaviour of distribution grids. It was then shown that performing transmission-level studies with those two equivalents gave similar confidence intervals as if full probabilistic transmission and distribution simulations were performed but in a much more practical manner. These equivalents have be shown to be adequate to model cascading outages and real power systems.

With those models built, chapter~\ref{ch:DPSA} has been able to develop a probabilistic dynamic security assessment methodology. This type of methodology generally consists in applying many contingencies into randomly sampled states of the grid, evaluating the consequences that would have such contingencies, and continuing until some statistical accuracy criterion is satisfied. In the literature, stopping criteria are often based on the standard error of the total risk, but in this thesis, it was argued that it is more useful to have an accurate estimate of the risk of individual contingencies. Indeed, most security-enhancing actions focus on solving local issues and thus affect the security of a limited number of contingencies. Then, a rigorous statistical accuracy indicator had to be developed to guarantee that no important unsecure regions are missed when sampling operating conditions. Based on this indicator, it was shown that when trying to have an accurate estimation of the risk of individual contingencies (as opposed to the total risk), the most effective sampling approach is generally a crude MC sampling, i.e. sampling of contingencies and operating conditions proportional to their probability of occurrence. To alleviate the computational burden of such crude MC approach, screening techniques and an implementation in a high-performance computing environment have been developed. The method has been applied to a 73-bus system and a scalability study has shown that the proposed method could be applied to large power grids with high but manageable computation burden.

The application of the methodology to a test system has also shown that a limited number of critical contingencies generally contribute to a large share of the total risk. In the considered test system, the 10 most critical delayed-clearing N-1 contingencies and N-2 contingencies (out of 708 considered contingencies) contributed to 44\% of the total risk. This indicates that complementing the N-1 criterion by securing a limited number of more severe contingencies could significantly reduce the risk and be cost-effective.

One drawback of probabilistic methods is that they require to perform many simulations which makes their results more difficult to interpret compared to deterministic methodologies. This drawback is partially alleviated by the fact that the analysts can focus on a small set of critical contingencies. In the considered test systems, 150,000 simulations were performed, but ``only'' a few hundreds were run for a given contingency. Moreover, it has been shown that by looking at which protection systems operate at the start of the simulated cascades, it is often possible to group those simulations in only a few different cascades sequences and to identify the root causes of insecurity. To help with the interpretation of the results, simple machine learning techniques have also been proposed to have an ``automatic'' identification of the root causes of the insecurity by identifying the boundary between secure and unsecure operating conditions for a given contingency. These identified security boundaries could also be used as operational rules to increase the security of the system at the cost of increased operating costs. One advantage of probabilistic methods is that these increased operating costs can be directly compared to the estimated reduction in unreliability costs (i.e. costs of occasionally having cascading outages and blackouts).

\section{Discussion}

The objective of this thesis was to develop a probabilistic dynamic security assessment methodology to cope with the limitations of deterministic methodologies. However, probabilistic methods do have drawbacks, some of which have not been properly acknowledged in the core of the thesis and are thus discussed here. Most importantly, probabilistic methodologies require significantly more data than deterministic ones, particularly reliability data: (weather-dependent) frequency of occurrence of faults, analysis of the failure modes of protection systems and estimation of their likelihood, etc. that require significant data collection campaigns over many years. However, without minimising the need for those campaigns, it should be noted that probabilistic security assessments can provide valuable insights even with incomplete data. Actually, in other fields (nuclear, aviation, oil and gas, etc.), it is common to perform probabilistic \emph{safety} assessments (PSA) with engineer best estimates for failure rates and other reliability data, at least in a first stage. Critical components can then be identified, and this, with sensitivity studies on the uncertain parameters, can help focussing data collection on the most important parameters.

Actually, in the nuclear sector, it sometimes said that the numerical value of the total risk estimated from a PSA is of little importance (despite being a regulatory target), and that what actually is the identification of critical components and the provision of recommendations on how to effectively reduce the risk. This statement might be more or less valid depending on whether one tries to minimise the risk following the as low as reasonably achievable (ALARA) approach (or as low as achievable with a fixed budget), or following a cost optimisation approach. In any case, it can be noticed that the ranking of critical contingencies or critical elements tends to be less sensitive to uncertainties than the estimation of risk itself. For example, if the average frequency of faults is not well estimated, it will affect the estimated risk of individual contingencies and estimated total risk, but not their ranking. Thus, without perfect data, probabilistic methodologies cannot fully live to their promise of allowing for the direct comparison of the cost of unreliability (blackouts\footnote{It can be noted that, even post-mortem, it is difficult to compute the societal costs of blackouts.}) and the cost of security-enhancing actions. But, they can provide strong indications on which security-enhancing actions are the most effective in reducing the risk, and they can give a rough indication on whether these actions are worth implementing.

The second main drawback of probabilistic methodologies, especially for dynamic security assessment, is the need for advanced modelling to be able to simulate the system during degraded conditions. However, this is also one of its main advantages. Simulating cascading outages allows one to get a better understanding of the behaviour of power systems and their entangled dynamics~\cite{PEGASE_simulation}, similarly to how many lessons can be learned from the post-mortem analysis of large disturbances. As for the first drawback, one should not expect to perform a perfect probabilistic security assessment in one go. However, as the first assessment already provides strong indications on which are the most critical contingencies, it also indicates which simulated scenarios should be analysed in more details. By analysing those scenarios, experts will be able to assess the adequacy of the models used that can then be improved for the next iterations of the probabilistic security assessment.

% 'Garbage In, Garbage Out' should not be taken to imply any sort of conservation law limiting the amount of garbage produced. \url{https://xkcd.com/2295/}, You never trully understand a system before you perform a PSA on it.


\section{Future work}

While this thesis has alleviated the main barriers limiting the use of probabilistic dynamic security assessment methodologies on real power systems, there is room for further work. Main directions for improvement are listed below.

\subsection*{Interpretation and use of the results}

Probabilistic security assessments perform hundreds of thousands of simulations, so it is difficult to extract all the useful information generated by those simulations. There is a plethora of security indicators (reviewed in~\cite{cypressD11}) that could help with this, however, they often focus on the consequences of cascading outages (overloading, undervoltage, load shedding), and not on their root causes which are more difficult to identify.

Conversely, this thesis has shown that tripping sequences allow for a first identification of the drivers of a cascading outage, and that they also allow grouping similar scenarios for a given contingency. The concept of ``critical event corridors'' proposed in DCAT~\cite{DCATphase1} goes a step further. A critical event corridor is a sequence of events that occurs in many system states and for many initial contingencies. The identification of such corridors would allow designing system integrity protection schemes that protect the grid against many unsecure scenarios. However, to the best of my knowledge, such corridors have never been identified and \cite{DCATphase1} does not provide with algorithm to identify them.

In another direction, the use of data mining and machine learning techniques to identify security boundaries and the root causes of cascading outages has only been superficially studied in this thesis, so further work could provide useful, especially regarding the identification of root causes. Also, as discussed in the thesis, the definition of simple security boundaries can help to make the link between the security assessment (based on a complex grid model and many scenarios) and security management methodologies such as security/risk constrained optimal power flows that require relatively simple models to converge.

% \item Coherence between risk management and risk assessment (DPSA): methods for risk assessment (especially this one) make intensive use of detailed simulations. On the other hand, risk management methods (e.g. SCOPF) are optimisation methods that require very simplified grid models to be solved in a reasonable amount of time. There are thus perspectives to

\subsection*{Application to operational planning}

The test cases considered in this thesis focused on long-term planning with simulations performed for system conditions representative of the whole year. It also briefly touched on real-time operation through the definition of machine-learning-based security boundaries. The proposed method could also be applied for operational planning for instance for the maintenance planning. It should however be noted that, since transmission assets typically have a high availability, few of the scenarios simulated in a long-term analysis might be reusable for the maintenance planning of a given asset (that is often available in the long-term scenarios, but forced unavailable for maintenance). The computational cost of performing a complete probabilistic security assessment from scratch each time an asset has to be put for maintenance might be prohibitive, so the number of considered contingencies for such assessment should be limited.

% \subsection*{Security enhancement and risk management}
%
% This thesis has proposed a method for probabilistic risk assessment which is a first step towards probabilistic security management.
%
% SPS

\subsection*{Modelling of unwanted trips}

In this thesis, missing trips were the only considered failure mode of protection systems. Unwanted trips are significantly harder to model but can also impact system security, especially when occurring as a consequence of another fault. The main causes of unwanted trips are the inadvertent use of wrong settings in protection relays and design errors which are extremely difficult to model and predict. It is also more difficult to obtain statistics on unwanted trips, as one has to distinguish between spontaneous unwanted trips (that occur without a preceding fault) and dependent unwanted trips, distinguish between voltage levels (with different protection philosophies), and cause of trip (that is not always reported, and often more complex than the reason for a missed trip).

% Unwanted: reporting: protection operates ``as designed'' (even if wrong settings), but should still be flagged as unwanted with reasons why it operated.

\subsection*{Application to large grids}

To facilitate the massive integration of renewable energy sources, power grids are becoming increasingly more connected which also means that cascading outages are more likely to propagate from one country to the other. The simulation of continental grids comes however with significant challenges, such as the data exchanges between TSOs, the scale of the simulated network and the heterogeneity between the grids of different TSOs~\cite{PEGASE_project}.

% Although the scalability of the proposed methodology has been discussed, the method should actually be applied to a large system to evaluate its computational cost. More importantly, the scalability has to des

\subsection*{Slow cascades}

While there is a growing share of purely-fast cascades, i.e. cascades that fully develop in at most a few minutes, there are still many cascades that start as slow cascades and latter transition to fast cascades if they cannot be stopped in the slow phase. There are two ways such cascades could be considered in a probabilistic security assessment. The first approach would be to use the same methodology as in this thesis but augment the models used to include slower dynamics (load models, over-excitation limiters, etc.). To keep acceptable computation time, this requires the use of simulators that can simulate large ranges of timescales thanks to a variable integration time step such as Eurostag~\cite{STAG} or Dynawo~\cite{Dynawo}. Alternatively, the slow and fast phases could be simulated separately as proposed in~\cite{TwoLevelPSA}, using a quasi-steady-state (QSS) simulator during the slow phase and a time-domain simulator during the fast phase. However, this requires predicting when a given cascade transition from the slow to the fast phase. Moreover, there is no consensus on how to model slow cascades in QSS simulations, so different QSS simulators tend to give different results~\cite{Benchmarking2018}. One often overlooked challenge when simulating slow cascades is to properly account for the actions of operators and the possibility for them to perform wrong or unoptimal actions due to stress or lack of situation awareness~\cite{Shahab_HRA, Panteli_Awareness}.

\subsection*{Load modelling}

Although the load modelling method used in this thesis can be applied to any kind of distribution network and assets. It can be reminded that, while not considered in the test cases in this thesis, the growing share of inverter-connected loads and in particular electric vehicles should be considered. Also, the parameters of the equivalents could be reused or updated instead of being recomputed from scratch when operating conditions change as proposed in~\cite{ChaspierreThesis}.
